{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apoyo a la explicación de algunos conceptos del tema 4 \n",
    "\n",
    "## Procesos de Decisión de Markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas definiciones Python permiten explicar, con un ejemplo, el concepto de valoración de un estado respecto de de una política dada (notado $V^{\\pi}(e)$), en el contexto de los Procesos de Decisión de Markov. \n",
    "\n",
    "Algunas de las definiciones que aquí veremos se solapan con el principio de la Práctica 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supondremos que un Procesos de Decisión de Markov (MDP, por sus siglas en inglés) va a ser un objeto de la siguiente clase (o mejor dicho, de una subclase de la siguiente clase). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(object):\n",
    "\n",
    "    \"\"\"La clase genérica MDP tiene como métodos la función de recompensa R,\n",
    "    la función A que da la lista de acciones aplicables a un estado, y la\n",
    "    función T que implementa el modelo de transición. Para cada estado y\n",
    "    acción aplicable al estado, la función T devuelve una lista de pares\n",
    "    (ei,pi) que describe los posibles estados ei que se pueden obtener al\n",
    "    aplicar la acción al estado, junto con la probabilidad pi de que esto\n",
    "    ocurra. El constructor de la clase recibe la lista de estados posibles y\n",
    "    el factor de descuento.\n",
    "\n",
    "    En esta clase genérica, las funciones R, A y T aparecen sin definir. Un\n",
    "    MDP concreto va a ser un objeto de una subclase de esta clase MDP, en la\n",
    "    que se definirán de manera concreta estas tres funciones\"\"\"  \n",
    "\n",
    "    def __init__(self,estados,descuento):\n",
    "\n",
    "        self.estados=estados\n",
    "        self.descuento=descuento\n",
    "\n",
    "    def R(self,estado):\n",
    "        pass\n",
    "\n",
    "    def A(self,estado):\n",
    "        pass\n",
    "        \n",
    "    def T(self,estado,accion):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideramos el siguiente problema del robot en la cuadrícula, descrito en la diapositiva 7 del tema 4. Lo que sigue es una definición de los estados, acciones y recompensa para ese problema en una clase python. No entraremos a dar los detalles sobre la siguiente implementación, pero baste decir que:\n",
    "\n",
    "\n",
    "* El constructor de la clase recibe la cuadrícula como una lista de listas de strings\n",
    "* Los atributos `estados` y `recompensa` contienen la lista de estados y la recompensa. A diferencia de las coordenadas usadas en las diapositivas, un estado vendrá referencias por el par (f,c), con la fila y columna en la que se encuentra (de arriba a abajo, de izquierda a derecha y comenzando en 0). \n",
    "* La recompensa de un estado la da el método `R`.\n",
    "* Las acciones aplicables a un estado se obtienen con el método `A`.\n",
    "* Los estados y las probabilidades que resultan de aplicar una acción a un estado, se obtienen con el método `T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cuadrícula(MDP):\n",
    "    \"\"\"Supondremos que cuadrícula viene dada por una lista de listas, en la\n",
    "    que si hay un número es la recompensa de esa casilla, si hay otra cosa la\n",
    "    recompesa es la que se indica por defecto y si hay None es una casilla\n",
    "    bloqueada. Supondremos que está bien construida\"\"\"\n",
    "\n",
    "    def __init__(self,cuadrícula,recompensa_por_defecto=-0.04,descuento=0.9,ruido=0.2):\n",
    "\n",
    "        estados=[]\n",
    "        terminales=[]\n",
    "        recompensa={}\n",
    "        nfilas=len(cuadrícula)\n",
    "        ncolumnas=len(cuadrícula[0])\n",
    "        for i in range(nfilas):\n",
    "            for j in range(ncolumnas):\n",
    "                contenido=cuadrícula[i][j]\n",
    "                if contenido != \"*\":\n",
    "                    estados.append((i,j))\n",
    "                    if isinstance(contenido,(int,float)):\n",
    "                        recompensa[(i,j)]=contenido\n",
    "                        terminales.append((i,j))\n",
    "                    else:\n",
    "                        recompensa[(i,j)]=recompensa_por_defecto\n",
    "                        if contenido=='S':\n",
    "                            estado_inicial=(i,j)\n",
    "                    \n",
    "        super().__init__(estados,descuento)\n",
    "        self.estados_terminales=terminales\n",
    "        self.nfilas=nfilas\n",
    "        self.ncolumnas=ncolumnas\n",
    "        self.recompensa=recompensa\n",
    "        self.ruido=ruido\n",
    "        self.desplazamientos={\"arriba\":[(-1,0),(0,-1),(0,1)],\n",
    "                                         \"abajo\":[(1,0),(0,-1),(0,1)],\n",
    "                                         \"izquierda\":[(0,-1),(-1,0),(1,0)],\n",
    "                                         \"derecha\":[(0,1),(-1,0),(1,0)]}\n",
    "\n",
    "\n",
    "    def R(self,estado):\n",
    "        return self.recompensa[estado]\n",
    "\n",
    "    def A(self,estado):\n",
    "        if estado in self.estados_terminales:\n",
    "            return [\"exit\"]\n",
    "        else:\n",
    "            return [\"arriba\",\"abajo\",\"izquierda\",\"derecha\"]\n",
    "\n",
    "    def T(self,estado,acción):\n",
    "\n",
    "        def desplaza(estado,despl):\n",
    "            x,y=estado\n",
    "            i,j=despl\n",
    "            nx,ny=x+i,y+j\n",
    "            if (nx,ny) in self.estados:\n",
    "                return nx,ny\n",
    "            else:\n",
    "                return x,y\n",
    "\n",
    "        if acción==\"exit\":\n",
    "            return([(estado,0)])\n",
    "        else:\n",
    "            despl=self.desplazamientos[acción]\n",
    "            pok=1-self.ruido\n",
    "            pnook=self.ruido/2\n",
    "            return [(desplaza(estado,despl[0]),pok),\n",
    "                      (desplaza(estado,despl[1]),pnook),\n",
    "                      (desplaza(estado,despl[2]),pnook)]\n",
    "            \n",
    "\n",
    "cuadrícula_1 = [[' ',' ',' ',+1],\n",
    "                [' ','*',' ',-1],\n",
    "                [' ',' ',' ',' ']]\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "cuad1_MDP=Cuadrícula(cuadrícula_1,descuento=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos ejemplos de los elementos que componen este MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (1, 0),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuad1_MDP.estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3), (1, 3)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuad1_MDP.estados_terminales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuad1_MDP.R((0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuad1_MDP.R((1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuad1_MDP.R((2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exit']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuad1_MDP.A((0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exit']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuad1_MDP.A((1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arriba', 'abajo', 'izquierda', 'derecha']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuad1_MDP.A((0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2, 1), 0.8), ((1, 2), 0.1), ((2, 2), 0.1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuad1_MDP.T((2,2),\"izquierda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2, 0), 0.8), ((2, 1), 0.1), ((2, 1), 0.1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuad1_MDP.T((2,1),\"izquierda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, dado un MDP, representaremos **una política** para el mismo como un diccionario cuyas claves son los estados, y los valores las acciones. Una política representa una manera concreta de decidir en cada estado la acción (de entre las aplicables a ese estado) que ha de aplicarse. \n",
    "\n",
    "Lo que sigue es nuestra representación de la política que aparece en el dibujo (a) de la izquierdo de la diapositiva 11 del tema 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi1={(0,0):\"derecha\",\n",
    "     (0,1):\"derecha\",\n",
    "     (0,2):\"derecha\",\n",
    "     (0,3):\"exit\",\n",
    "     (1,0):\"arriba\",\n",
    "     (1,2):\"arriba\",\n",
    "     (1,3):\"exit\",\n",
    "     (2,0):\"arriba\",\n",
    "     (2,1):\"izquierda\",\n",
    "     (2,2):\"izquierda\",\n",
    "     (2,3):\"izquierda\"}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado un MDP, un estado de partida, y una política concreta, podemos generar (muestrear) una secuencia de estados por los que iríamos pasando si vamos aplicando las acciones que nos indica la política: dado un estado de la secuencia, aplicamos a ese estado la acción que indique la política, y obtenemos un estado siguiente de manera aleatoria, pero siguiendo la distribución de probabilidad que indica el modelo de transición dado por el método T.  \n",
    "\n",
    "La siguiente función `genera_secuencia_estados(mdp,pi,e,n)` devuelve una secuencia de estados de longitud n (menor si se llega antes a un estado terminal), obtenida siguiendo el método anterior. Aquí mdp es objeto de la clase MDP, pi es una política, e un estado de partida y n la longitud de la secuencia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución\n",
    "\n",
    "# distr es un lista de pares (vi,pi) con los diferentes valores de la v.a. y\n",
    "# sus probabilidades. \n",
    "def muestreo(distr):\n",
    "    r=random.random()\n",
    "    acum=0\n",
    "    for v,p in distr:\n",
    "        acum+=p\n",
    "        if acum>r:\n",
    "            return v\n",
    "            \n",
    "def genera_secuencia_estados(mdp,pi,e,n):\n",
    "    actual=e\n",
    "    seq=[actual]\n",
    "    for _ in range(n-1):\n",
    "        if actual in mdp.estados_terminales:\n",
    "            break\n",
    "        actual=muestreo(mdp.T(actual,pi[actual]))\n",
    "        seq.append(actual)\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos a continuación algunos ejemplo de uso. Está claro que partiendo desde el mismo estado y usando siempre la misma política no tiene por qué salir siempre la misma secuencia, debido a la incertidumbre que se tiene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 2), (2, 1), (2, 0), (2, 0), (1, 0), (0, 0), (0, 1), (0, 2), (0, 3)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genera_secuencia_estados(cuad1_MDP,pi1,(2,2),15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 2),\n",
       " (2, 1),\n",
       " (2, 0),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 2),\n",
       " (0, 3)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genera_secuencia_estados(cuad1_MDP,pi1,(2,2),15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 2), (1, 2), (0, 2), (0, 3)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genera_secuencia_estados(cuad1_MDP,pi1,(2,2),15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado un MDP y una secuencia de estados, valoramos dicha secuencia como la suma de las recompensas de los estados de la secuencias (cada una con el correspondiente descuento). Se pide definir una función `valora_secuencia(seq,mdp)` que calcula esta valoración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valora_secuencia(seq,mdp):\n",
    "    return sum(mdp.R(e)*(mdp.descuento**i) for i,e in enumerate(seq))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001326592000000043"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valora_secuencia([(2, 2), (2, 1), (2, 0), (1, 0), (0, 0), (0, 0), (0, 1), (0, 2), (0, 3)],cuad1_MDP)\n",
    "\n",
    "# Resultado:\n",
    "# 0.001326592000000043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4144000000000001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valora_secuencia([(2, 2), (1, 2), (0, 2), (0, 3)],cuad1_MDP)\n",
    "# Resultado:\n",
    "# 0.4144000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11753662791680003"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valora_secuencia([(2, 2),(2, 2),(2, 2),(2, 1),(2, 1),(2, 1),(2, 0),\n",
    "                 (1, 0),(1, 0),(0, 0),(0, 1),(0, 2),(0, 3)],cuad1_MDP)\n",
    "\n",
    "# Resultado:\n",
    "# -0.11753662791680003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que comenzando en un estado dado y aplicando una política dada, uno puede obtener distintas secuencias, esta claro que para definir el valor que espero obtener a partir de ese estado, debo recurrir a la noción de valor medio esperado, que es lo que se explica acontinuación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada una política $\\pi$, la valoración de un estado e respecto de esa política, $V^{\\pi}(e)$, se define como la media esperada de las valoraciones de las secuencias que se pueden generar teniendo dicho estado como estado de partida. \n",
    "\n",
    "Usando las funciones anteriores, se puede define una función `estima_valor(e,pi,mdp,m,n)` que realiza una estimación aproximada del valor $V^{\\pi}(e)$: para ello genera $n$ secuencias de tamaño $m$, y calcula la media de sus valoraciones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución:\n",
    "\n",
    "def estima_valor(e,pi,mdp,m,n):\n",
    "    return (sum(valora_secuencia(genera_secuencia_estados(mdp,pi,e,m),mdp) \n",
    "                for _ in range(n)))/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuad2_MDP=Cuadrícula(cuadrícula_1,descuento=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125095999997521"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor((0,0),pi1,cuad2_MDP,15,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5998559999998727"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor((2,2),pi1,cuad2_MDP,15,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9184796000009078"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor((0,2),pi1,cuad2_MDP,15,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3664759999998041"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor((2,3),pi1,cuad2_MDP,15,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6614579999988486"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estima_valor((1,2),pi1,cuad2_MDP,15,100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos calculado $V^{\\pi}(e)$ mediante una función que ha simulado las secuencias de estados en un MDP, y haciendo la correspondiente media. Pero en las diapositivas del tema veremos que se puede calcular $V^{\\pi}(e)$ resolviendo un sistema de ecuaciones lineales.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
